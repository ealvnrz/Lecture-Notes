---
title: Machine Learning
subtitle: 
author: Eloy Alvarado Narváez
institute: Instituto de Estadística \newline Universidad de Valparaíso
titlegraphic: logo.png
fontsize: 10pt
output:
 beamer_presentation:
    template: beamer_template.tex
    keep_tex: true
    toc: false
    latex_engine: pdflatex
    slide_level: 3
    highlight: arrow
make149: true
lang: es
#mainfont: "Open Sans" # Try out some font options if xelatex
#fontfamily: firasans
titlefont: "Titillium Web" # Try out some font options if xelatex
---

```{r setup, include=FALSE, cache=F, message=F, warning=F, results="hide"}
knitr::opts_chunk$set(cache=TRUE)
knitr::opts_chunk$set(fig.path='figs/')
knitr::opts_chunk$set(cache.path='cache/')

knitr::opts_chunk$set(
                  fig.process = function(x) {
                      x2 = sub('-\\d+([.][a-z]+)$', '\\1', x)
                      if (file.rename(x, x2)) x2 else x
                      }
                  )
library(tidyverse)
```

# Introducción

-   El problema de buscar patrones
-   El reconocimiento de patrones se ocupa del descubrimiento automático de regularidades en los datos mediante el uso de algoritmos, y usa estas regularidades para tomar acciones.

### Ejemplo

Tomemos como ejemplo el reconocer dígitos escritos a mano.

![Ejemplos de dígitos escritos a mano tomados desde códigos postales](./figs/zip_codes.png)

------------------------------------------------------------------------

Estos dígitos corresponden a imágenes de 28x28 pixeles, por lo que pueden ser representados en un vector $\mathbf{x}$ que contiene 784 números reales.

El objetivo es construir una **máquina** que tome el vector $\mathbf{x}$ como entrada y produzca la identidad del dígito $0,\dots,9$ como salida.

Este problema es claramente no-trivial debido a la gran variedad de escrituras. Podría abordarse utilizando reglas heurísticas para distinguir los dígitos en función de las formas de los trazos, pero en la práctica, tal enfoque conduce a una proliferación de reglas y de excepciones a las reglas, etc., e invariablemente da malos resultados.

------------------------------------------------------------------------

Mejores resultados pueden ser obtenidos adoptando un enfoque de **meachine learning**, en donde un conjunto grande de datos de $N$ dígitos $\{x_1 ,\ldots, x_n\}$ llamados **conjunto de entrenamiento (training set)** se utiliza para ajustar los parámetros de un modelo adaptativo.

Las categorías de los dígitos en el conjunto de entrenamiento se conocen de antemano, normalmente inspeccionándolos individualmente y etiquetándolos a mano.

Podemos expresar la categoría de un dígito usando un **vector objetivo (target vector)** $\mathbf{t}$, que representa la identidad del dígito correspondiente. Notar que hay un vector objetivo $\mathbf{t}$ para cada dígito de la imágen $\mathbf{x}$.

------------------------------------------------------------------------

El resultado tras aplicar el algoritmo de **machine learning** puede ser expresado como una functión $\mathbf{y}(\mathbf{x})$, que toma una nueva imagen del dígito $\mathbf{x}$ como entrada y que genera como salida un vector $\mathbf{y}$, codificada de la misma manera que los vector objetivos.

La forma exacta de la función $\mathbf{y}(\mathbf{x})$ es determinada durante la **fase de entrenamiento**, también conocida como la fase de aprendizaje, en base al conjunto de entrenamiento.

Una vez que el modelo es entrenado, este puede ser usado para identificar nuevas imágenes de dígitos, que les llamamos **conjunto de prueba (test set)**.

La habilidad de categorizar correctamente nuevos ejemplos que difieren de los utilizados en la fase de aprendizaje es conocido como **generalización**.

------------------------------------------------------------------------

En la mayoría de las aplicaciones reales, las variables de entrada son típicamente preprocesadas para transformarlas a un n uevo espacio de variables donde, se espera que la problemática de reconocer patrones sea más fácil de resolver.

Por ejemplo, en el reconocimiento de dígitos escritos a mano, las imágenes de los dígitos generalmente se transforman y escalan tal que cada dígito esté contenido dentro de un cuadro de tamaño fijo. Esto reduce en gran medida la variabilidad dentro de cada clase de dígito, debido a que la localización y la escala de todos los dígitos serán las mismas, por lo que la identificación de patrones se facilitará.

La etapa de de **pre-procesamiento** es usualmente conocida como **extracción de características (feature extraction)**.

Notar que los nuevos datos, incluidos en el conjunto de entrenamiento, deben ser preprocesados de igual manera que los del conjunto de entrenamiento.

------------------------------------------------------------------------

La etapa de preprocesamiento también puede ser utilizada para acelerar el cálculo del algoritmo utilizado. Se debe tener especial cuidado en esta etapa debido a que usualmente, cierta información es descartada, y si esta es imporatnte para la solución del problema, la precisión general del sistema confeccionado puede verse afectada.

Las aplicaciones en donde la entrada son los datos de entrenamiento (training set) en conjunto con sus correspondientes vectores objetivo son conocidas como **problemas de aprendizaje supervisado (supervised learning problems)**.

Los casos en donde el objetivo es asignar a cada vector de entrada una categoría, se conocen como **problemas de clasificación**.

Si se desean salidas que consisten en una o más variables continuas, entonces le llamamos **regresión**.

------------------------------------------------------------------------

Las aplicaciones en donde la entrada son los datos de entrenamiento (training set) sin sus correspondientes vectores objetivos son conocidas como **problemas de aprendizaje no supervisado (unsupervised learning problems)**. Varios pueden ser los objetivos en este tipo de problemas:

-   Descubrir grupos de elementos similares dentro de los datos, en este caso le llamamos **agrupamiento (clustering)**
-   Estimar la distribución de los datos dentro del espacio de los datos, a esto le llamamos **estimación de densidad**
-   Proyectar los datos desde un espacio multidimensional a uno de 2 o 3 dimensiones, para así poder visualizarlo, a esto le llamamos **visualización**.

------------------------------------------------------------------------

Otra técnica utilizada en **machine learning** es el **aprendizaje reforzado (reinforcement learning)**, que se ocupa del problema de encontrar acciones adecuadas para tomar en una situación específica con el fin de maximizar una recompensa.

En este caso, el algoritmo de aprendizaje no recibe ejemplos de resultados óptimos (como se tienen en el aprendizaje supervisado), sino que debe descubrirlos mediante un proceso de prueba y error.

## Optimización no lineal

La forma estándar de un problema de optimización no lineal es:

```{=tex}
\begin{align*}
\min_{x}\,& f(x) \\
\text{donde } & g_1(x)  \leq 0\\
& \vdots \\
& g_l(x)  \leq 0\\
& h_1(x)  = 0 \\
& \vdots \\
& h_m(x)  = 0 \\
\end{align*}
```
$f(x)$ le llamamos la función objetivo, usualmente a minimizar. Todas las otras restricción son de la forma $\leq$ o $=$.

## Conjunto convexo

El problema **general** de optimización no lineal (donde, $f,g$ y $h$ pueden ser cualquier función) es extremadamente dificil de resolver. Sin embargo, si la función objetivo y las restricción son lo suficientemente *buenas*, existen algoritmos eficientes para encontrar un mínimo global.

Una de estas *buenas* condiciones, es la **convexidad**.

Existen dos definiciones para convexidad, una para conjuntos y otra para funciones. Intuitivamente, un conjunto convexo no tiene ningún agujero.

![](./figs/convex_sets.png)

------------------------------------------------------------------------

Una definición más precisa es:

**Para dos puntos cualesquiera del conjunto, la línea recta que conecta esos dos puntos también se encuentra en el conjunto**.

Especificamente, El conjunto $X$ es convexo si, para cualquier $x_1\in X, x_2 \in X$, y $\lambda \in [0,1]$, el punto $\lambda x_1 + (1-\lambda)x_2 \in X$ (este punto es una combinación convexa de $x_1$ y $x_2$).

-   ¿El plano $X=\{ (x,y,z): 3x+4y-3z=1\}$ es convexo?

-   ¿Es la región $X=\{ (x,y): x^2+y^2\geq 1\}$ convexa?

**Para mostrar que un conjunto es convexo, se debe mostrar que toda combinación convexa de dos puntos en el conjunto está dentro del conjunto**.

**Para mostrar que un conjunto no es convexo, basta mostrar un caso en donde no suceda**.

## Funciones convexas

Una definición clásica que se da en cálculo (aunque acotada), es que una función unidimensional, diferenciable dos veces, es convexa si $f''(x)\geq 0$ en todo punto.

Ahora, generalizaremos este definición a más dimensiones, y a funciones que no son dos veces diferenciables.

Una función $f:X\rightarrow \mathbb{R}$ es **convexa** si, para cada $x_1,x_2 \in X$ y cada $\lambda \in (0,1)$,

$$f((1-\lambda)x_1+\lambda x_2))\leq (1-\lambda)f(x_1)+\lambda f(x_2)$$ Si la desigualdad es estricta, entonces se llama **estrictamente convexa**.

------------------------------------------------------------------------

```{=tex}
\begin{tikzpicture}
\begin{axis}[width=5in,axis equal image,
    axis lines=middle,
    xmin=0,xmax=8,
    xlabel=$x$,ylabel=$y$,
    ymin=-0.25,ymax=4,
    xtick={\empty},ytick={\empty}, axis on top
]

% 
\addplot[thick,domain=0.25:7,blue,name path = A]  {-x/3 + 2.75} coordinate[pos=0.4] (m) ;
\draw[thick,blue, name path =B] (0.15,4) .. controls (1,1) and (4,0) .. (6,2) node[pos=0.95, color=black, right]  {$f(x)$} coordinate[pos=0.075] (a1)  coordinate[pos=0.95] (a2);
\path [name intersections={of=A and B, by={a,b}}];

% 
\draw[densely dashed] (0,0) -| node[pos=0.5, color=black, label=below:$a$] {}(a1);
\draw[densely dashed] (0,0) -| node[pos=0.5, color=black, label=below:$x_{1}$] {}(a);
\draw[densely dashed, name path=D] (3,0) -|node[pos=0.5, color=black, label=below:$\lambda x_{1}+ (1-\lambda)x_{2}$] {} node[pos=1, fill,circle,inner sep=1pt] {}(m);
\draw[densely dashed] (0,0) -|node[pos=0.5, color=black, label=below:$x_{2}$] {}(b);
\draw[densely dashed] (0,0) -|node[pos=0.5, color=black, label=below:$b$] {}(a2);

% 
\path [name intersections={of=B and D, by={c}}] node[fill,circle,inner sep=1pt] at (c) {}; 

% 
\node[anchor=south west, text=black] (d) at (0.75,3) {$f[\lambda x_{1}+(1-\lambda)x_{2}]$};
\node[anchor=south west, text=black] (e) at (5,2.5) {$\lambda f(x_{1})+(1-\lambda)f(x_{2})$};
\draw[-{Latex[width=4pt,length=6pt]}, densely dashed] (d) -- (c);
\draw[-{Latex[width=4pt,length=6pt]}, densely dashed] (e) -- (m);
\end{axis}
\end{tikzpicture}
```
-   ¿Es $f(x)=|x|$ convexa?

------------------------------------------------------------------------

Esta definición puede ser difícil de manejar, por lo que hay una caracterización alternativa.

Si la función es diferenciable, la convexidad puede ser caracterizada en términos de rectas tangentes a la función.

**La función** $f$ es convexa si está sobre todas sus rectas tangentes.

Matemáticamente, si $f$ es diferenciable en su dominio, entonces $f$ es convexa si y solo si

$$f(x_2)\geq f(x_1)+f'(x_1)(x_2-x_1)$$

para todo $x_1,x_2 \in X$.

-   ¿Es $x^2$ convexa?

Si $f$ es dos veces diferenciable en su dominio, entonces $f$ es convexa si y solo si $f''(x)\geq 0$ en todas partes.

------------------------------------------------------------------------

Cuando $f$ es una función de múltiples variables, las condiciones de convexidad que involucran la primera y segunda derivada deben cambiar.

El análogo a la primera derivada es el **vector gradiente**.

$$\nabla f=[\partial f / \partial x_1 \quad \partial f / \partial x_2 \cdots \partial f / \partial x_n]^T$$

El análogo de la segunda derivada es la **matrix Hessiana**.

$$H_f=\left[\begin{array}{cccc}
\partial^{2} f / \partial x_{1}^{2} & \partial^{2} f / \partial x_{1} \partial x_{2} & \cdots & \partial^{2} f / \partial x_{1} \partial x_{n} \\
\partial^{2} f / \partial x_{2} \partial x_{1} & \partial^{2} f / \partial x_{2}^{2} & \cdots & \partial^{2} f / \partial x_{2} \partial x_{n} \\
\vdots & \vdots & \ddots & \vdots \\
\partial^{2} f / \partial x_{n} \partial x_{1} & \partial^{2} f / \partial x_{n} \partial x_{2} & \cdots & \partial^{2} f / \partial x_{n}^{2}
\end{array}\right]$$

------------------------------------------------------------------------

Para funciones multidimensionales dos veces diferenciables, $f$ es convexa si cualquier de estas condiciones equivalentes se satisface.

1.  Para todo $x_1$ y $x_2$ en $X$ $$f(\lambda x_2+ (1-\lambda)x_1)\leq \lambda f(x_2)+(1-\lambda)f(x_1)$$

2.  Para todo $x_1$ y $x_2$ en $X$. $$f(x_2)\geq f(x_1)+\nabla f(x_1)^T(x_2-x_1)$$

3.  Para todo $x \in X, H(x)$ es semidefinida positiva (esto es, $y^T H(x)y\geq 0$ para todos los vectores $y$).

------------------------------------------------------------------------

Hay ciertas propiedades que se cumplen para las funciones convexas:

-   Cualquier función lineal es convexa
-   Un múltiplo no negativo de una función convexa es convexa
-   La suma de funciones convexas es convexa
-   La composición de funciones convexas es convexa.

Un problema de optimización convexa, es un problema de optimización en donde la función objetivo es una función convexa, y la región factible es un conjunto convexo.

### Método de Lagrange

La idea del método de Lagrange o más usualmente conocido como multiplicadores de Lagrange, es mover las restricciones hacia la función objetivo, y luego resolver como si fuese un problema sin restricciones.

```{=tex}
\begin{align*}
\min \quad -x_1-x_2 &\\
\text{sujeto a} \quad x_{1}^{2}+x_{2}^{2}-1&=0
\end{align*}
```
¿Cómo solucionamos este problema?

------------------------------------------------------------------------

Multiplicamos la restricción por $\lambda$ y luego la agregamos a la función objetivo para formar la función lagrangiana:

$$\mathcal{L}(x_1,x_2,\lambda)=-x_1-x_2+\lambda(x_{1}^{2}+x_{2}^{2}-1)$$

Los puntos estacionarios de esta función son los puntos en donde todas sus derivadas parciales son cero.

\begin{align*}
\dfrac{\partial \mathcal{L}}{\partial x_1}&=-1+2\lambda x_1=0\\
\dfrac{\partial \mathcal{L}}{\partial x_x}&=-1+2\lambda x_2=0\\
\dfrac{\partial \mathcal{L}}{\partial \lambda}&=x_{1}^{2}+x_{2}^{2}-1=0\\
\end{align*} Notar que la tercera ecuación nos entrega las restricciones iniciales.

------------------------------------------------------------------------

Estas ecuaciones se resuelven cuando $x_1=x_2=\lambda=1/\sqrt{2}$.

Así, la solución óptima del problema original es $x_1=x_2=1/\sqrt{2}$

Si hay más de una restricción, se introduce un multiplicador adicional diferente para cada una de estas.

### Tarea

Considere el siguiente problema de optimización

```{=tex}
\begin{align*}
\min \quad x^2+y^2+z^2 &\\
\text{sujeto a} \quad x^2+y^2-z^2&=0\\
x-2z-3&=0
\end{align*}
```
## Ejemplo

Se desea mejorar las ventas de un producto en particular. El siguiente conjunto de datos contiene datos de las ventas de aquel producto en 200 mercados diferentes, junto con el presupuesto de publicidad para el producto en cada uno de los mercados para 3 medios de publicidad: TV, radio y diario.

```{r, message=FALSE, warning=FALSE}
library(ISLR)
library(ggplot2)
library(gridExtra)
Advertising <- read.csv("./db/Advertising.csv")
head(Advertising)
```

------------------------------------------------------------------------

\small

```{r,fig.show=FALSE, message=FALSE}
p1<- ggplot(data = Advertising, mapping = aes(x = TV, y = Sales))+
    geom_point() + geom_smooth(method = "lm", se = FALSE)
p2<- ggplot(data = Advertising, mapping = aes(x = Radio, y = Sales))+
    geom_point() + geom_smooth(method = "lm", se = FALSE)
p3<- ggplot(data = Advertising, mapping = aes(x = Newspaper, y = Sales))+
    geom_point() + geom_smooth(method = "lm", se = FALSE)
```

------------------------------------------------------------------------

```{r,message=FALSE}
grid.arrange(p1, p2, p3, nrow = 1)
```

------------------------------------------------------------------------

En este ejemplo, los presupuestos son las variables de entrada (**input**) mientras que las ventas es la variable de salida (**output**). Usualmente denotaremos a las variables de entrada por la letra $X$, así $X_1$ es el presupuesto en televisión, $X_2$ en Radio y $X_3$ en periódicos.

Estas variables de entregada también se le conocen como **predictores, variables independientes, *features*** o simplemente **variables**.

La variable respuesta **Sales** es usualmente llamada **respuesta** o **variable dependiente**, y se denota por la leta $Y$.

------------------------------------------------------------------------

En general, supongamos que observamos una variable respuesta cuantitative $Y$ y $p$ diferentes predictores $X_1,\dots,X_p$. Asumiremos que existe algún tipo de relación entre $Y$ y $X=(X_1,X_2,\dots,X_p)$ que puede ser escrito de forma general como

$$Y=f(X)+\varepsilon$$

Donde $f$ es una función fija de $X_1,\dots,X_p$ y $\varepsilon$ es un error aleatorio, que es independiente de $X$ y tiene media cero. En lo anterior, $f$ representa la información sistemática que $X$ provee sobre $Y$.

## Aprendizaje estadístico

El aprendizaje estadístico refiere al conjunto de herramientas y enfoques para **estimar** $f$.

**¿Para qué estimar** $f$?

### Predicción

En muchas situaciones, un conjunto de variables de entrada $X$ son fácilmente obtenibles, pero las salidas $Y$ tienen difícil acceso. Bajo esta configuración, debido a que el promedio de los errores tiene media cero, podemos predecir $Y$ usando:

$$
\hat{Y}=\hat{f}(X)
$$

donde $\hat{f}$ representa nuestra estimación para $f$ e $\hat{Y}$ representa la predicción obtenida para $Y$. En este contexto, $\hat{f}$ es usualmente tratada como una **caja negra**, en el sentido que no estamos usualmente preocupados con la forma exacta de $\hat{f}$, si es que esta entrega predicciones precisas de $Y$.

------------------------------------------------------------------------

\small

```{r}
library(plot3D)
Income2<- read.csv("./db/Income2.csv")
# Ajuste
fit_2_3_loess <- loess(Income ~ Education + Seniority, data = Income2) 
# Predicción de valores
x.pred <- seq(min(Income2$Education), max(Income2$Education), length.out = 30)
y.pred <- seq(min(Income2$Seniority), max(Income2$Seniority), length.out = 30)
xy     <- expand.grid(Education = x.pred, Seniority = y.pred)
z.pred <- matrix(predict(fit_2_3_loess, newdata = xy), nrow = 30, ncol = 30)
```

------------------------------------------------------------------------

\small

```{r,fig.show='hide'}
Income2 %>% 
  scatter3D(
    type = "p",
    x = Income2$Education, 
    y = Income2$Seniority, 
    z = Income2$Income,
    colvar = NA, pch = 19, col = "gold", cex = 1.75,
    phi = 25, theta = 45, expand = 0.6,
    xlab = "Years of Education", ylab = "Seniority", zlab = "Income",
    panel.first = scatter3D(x = Income2$Education,y = Income2$Seniority,
    z = Income2$Income,colvar = NA, col = "black", add = T,
    surf = list(x = x.pred, y = y.pred, z = z.pred, 
    fit = predict(fit_2_3_loess), facets = T, col = "skyblue",
    border = "royalblue", alpha = 0.45)))
```

------------------------------------------------------------------------

\small

```{r, echo=FALSE}
Income2 %>% 
  scatter3D(
    type = "p",
    x = Income2$Education, 
    y = Income2$Seniority, 
    z = Income2$Income,
    colvar = NA, pch = 19, col = "gold", cex = 1.75,
    phi = 25, theta = 45, expand = 0.6,
    xlab = "Years of Education", ylab = "Seniority", zlab = "Income",
    panel.first = scatter3D(x = Income2$Education,y = Income2$Seniority,
    z = Income2$Income,colvar = NA, col = "black", add = T,
    surf = list(x = x.pred, y = y.pred, z = z.pred, 
    fit = predict(fit_2_3_loess), facets = T, col = "skyblue", border = "royalblue", alpha = 0.45)))
```

------------------------------------------------------------------------

Consideremos que un estimador $\hat{f}$ y un conjunto de variables $X$ entregan la predicción $\hat{Y}=\hat{f}(X)$ . Asumiendo que $\hat{f}$ y $X$ son fijos, entonces se tiene:

```{=tex}
\begin{align*}
\mathbb{E}(Y-\hat{Y})^2 &= \mathbb{E}(f(X)+\varepsilon - \hat{f}(X))^2 \\
&= \underbrace{[f(X) - \hat{f}(X)]^2}_\text{Reducible} + \underbrace{\mathbb{V}(\varepsilon)}_\text{Irreducible}
\end{align*}
```
Nosotros nos concentraremos en técnicas para estimar $f$ con el fin de poder minimizar el error reducible.

### Inferencia

Usualmente estamos interesados en entender la forma en que $Y$ se ve afectada conforme $X_1,\dots,X_p$ cambia. En este tipo de situaciones, deseamos estimar $f$, pero nuestro objetivo no es necesariamente hacer predicciones para $Y$. En cambio, se quiere entender la relación entre $X$ e $Y$, por lo que ya no podemos tratar $\hat{f}$ como una caja negra, debido a que para poder explicar el fenómeno debemos tener una **forma exacta**. Usualmente nos preguntamos:

-   ¿Qué predictores están asociados con la respuesta?

-   ¿Cuál es la relación entre la respuesta y cada predictor?

-   ¿La relación entre $Y$ y cada predictor ser explicada adecuadamente usando una ecuación lineal o la relación es más complicada?

### ¿Cómo estimamos $f$?

A lo largo del curso, veremos enfoques lineales y no lineales para estimar $f$. Estos métodos usualmente comparten ciertas características.

En general, la mayoría de las técnicas de aprendizaje estadístico pueden ser categorizadas como **paramétricas** o **no-paramétricas**.

### Métodos paramétricos

Este enfoque tiene dos pasos y se base en modelos que reducen el problema de estimar $f$ a estimar un conjunto de parámetros.

**Pros**

-   Es mucho más fácil que ajustar una función arbitraria cualquiera

**Contras**

-   El modelo usualmente no seguirá la forma real de $f$

-   Si el ajuste está muy lejano a la forma real, la estimación será mala

-   Se puede caer en sobreajuste

------------------------------------------------------------------------

¿Cuales serían los pasos de un enfoque paramétrico?

1.  Asumir la forma de $f$
2.  Realizar un proceso que ajuste el conjunto de datos (**training set**) para el modelo

### Métodos no paramétricos

El enfoque no paramétrico se caracteriza por no asumir la forma de $f$, pero en lugar de eso intenta obtener una estimación de $f$ que sea lo más cercano al conjunto de datos sin llegar a un sobreajuste.

**Pros**

-   Al no asumir nada sobre $f$, estos métodos permiten un vasto rango de formas que se ajustan con precisión a $f$

**Contras**

-   Un gran número de datos es necesario para estimar de forma precisa $f$, mucho más que bajo un enfoque paramétrico.

## Compensación entre precisión vs interpretabilidad

Como sabemos hay métodos de aprendizaje estadístico que son menos flexibles que otros, por ejemplo la regresión lineal. Sin embargo, existen razones para escoger estas metodologías en vez de una más flexible.

-   Si la inferencia es nuestro principal objetivo, los modelos más restrictivos son recomendados debido a que la relación entre $X$ e $Y$ es fácilmente interpretable.

-   Métodos más flexibles usualmente llegar a estimación más complejas que dificultan el análisis de alguna relación individual entre un predictor y la variable respuesta.

-   Incluso cuando la predicción es el único objetivo, modelos más restrictivos pueden entregar mayor precisión que la mayoría de los métodos más flexible, debido a que estos últimos pueden sobreajustar.

------------------------------------------------------------------------

![](figs/inter_flex.png)

## Teorema del No-Free-Lunch

¿Por qué no simplemente elegimos el **mejor** método para todos los problemas?

El teorema de No-Free-Lunch establece que todos los algoritmos de optimización se desempeñan igualmente bien cuando su desempeño es promediado sobre todas las funciones objetivos posibles.

## Compromiso sesgo-varianza

Una de las herramientas que tenemos para cuantificar que tan bueno es nuestro ajuste es el Error cuadrático medio, lo notamos por sus siglas en inglés **MSE**. Para un valor $x_0$ dado, es posible mostrar que el error cuadrático medio se puede descomponer de la forma

$$
\mathbb{E}\left(y_0 - \hat{f}(x_0)\right)^2=\mathbb{V}(\hat{f}(x_0))+[Bias(\hat{f}(x_0))]^2+\mathbb{V}(\varepsilon)
$$

En donde el lado izquierdo representa el error cuadrado medio esperado cuando se estima $f$ y se evalúan en el punto $x_0$.

De la ecuación anterior se desprende que para minimizar el error cuadrático medio se debe seleccionar una metodología que simultáneamente logre una varianza baja y un bajo sesgo.

------------------------------------------------------------------------

A esta relación le llamamos un compromiso, debido a que es fácil obtener un método con extremadamente bajo sesgo pero varianza alta o un modelo con baja varianza pero alto sesgo.

Como regla general, si se utilizan metodologías más flexibles, la varianza crecerá y el sesgo disminuirá.

# Métodos supervisados

Como hemos mencionado a lo largo del curso, una regresión lineal simple asume que la variable respuesta $Y$ es **cuantitativa**, pero en muchas situaciones esta es **cualitativa** (también referida como categórica). En lo que sigue, veremos métodos para predecir respuestas cualitativas, más comúnmente llamado **clasificación**.

Existen mucha técnicas de clasificación o **clasificadores**, que se pueden usar para predecir una variable cualitativa. Entre ellos se encuentras:

-   Regresión logística

-   Análisis discriminante lineal

-   *k-NN (k- nearest neighbors / k-vecinos cercanos)*

-   Modelos generalizados aditivos

-   Árboles y bosques aleatorios

-   Boosting

-   SVM

### Ejemplo

```{r}
data<-Default
head(data)
```

------------------------------------------------------------------------

```{r, fig.show='hide'}
ggplot(data) +
 aes(x = balance, y = income, colour = default) +
 geom_point(shape = "bullet", size = 1.5) +
 scale_color_hue(direction = -1) +
 theme_gray()

```

------------------------------------------------------------------------

```{r, echo=FALSE}
ggplot(data) +
 aes(x = balance, y = income, colour = default) +
 geom_point(shape = "bullet", size = 1.5) +
 scale_color_hue(direction = -1) +
 theme_gray()
```

------------------------------------------------------------------------

```{r, echo=FALSE}
ggplot(data) +
  aes(x = default, y = balance, fill = default) +
  geom_boxplot(shape = "circle") +
  scale_fill_hue(direction = -1) +
  theme_gray()
```

------------------------------------------------------------------------

```{r, echo=FALSE}
ggplot(data) +
  aes(x = default, y = income, fill = default) +
  geom_boxplot(shape = "circle") +
  scale_fill_hue(direction = -1) +
  theme_gray()

```

### ¿Por qué no usar una regresión lineal?

Supongamos que se intenta predecir la condición médica de un paciente en la sala de emergencia con base a sus síntomas. Para simplificar, imaginemos que sólo que tienen 3 posibles diagnósticos: accidente cardiovascular, sobredosis y ataque epiléptico. Por lo que podríamos clasificar la variable respuesta como

$$
Y=\begin{cases} 1 \quad \text{si Accidente cardiovascular}\\
2 \quad \text{si Sobredosis} \\
3 \quad \text{si Ataque epiléptico}
\end{cases}
$$

Usando esta codificación, se puede usar el método de mínimos cuadrados para ajustar una regresión lineal para predecir $Y$ en base a los predictores $X_1,\dots, X_p$.

------------------------------------------------------------------------

Desafortunadamente, esta codificación implica un ordenamiento de las salidas, estableciendo sobredosis entre accidente cardiovascular y Ataque epiléptico, e inherentemente afirmando que la diferencia entre categorías contiguas son la misma.

Es claro notar que si usamos otra codificación, el ajuste de regresión lineal obtenido será diferente al primero. En general, no hay una forma natural de convertir una variable respuesta cualitativa con más de dos niveles en una variable cuantitativa que esté lista para hacer una regresión lineal.

------------------------------------------------------------------------

En el caso de variable respuesta binaria, la situación es algo más favorable, debido a que si se cambia la codificación, el ajuste de regresión obtenido será el mismo. Sin embargo, el método de mínimos cuadrados no tiene sentido, provocando que algunas de nuestras estimación estén fuera del intervalo [0,1], haciendo difícil la interpretación de las probabilidades.

Lo anterior debido a que se puede mostrar que el $X\hat{\beta}$ obtenido con la regresión lineal con codificación binaria, es simplemente una estimación de $\mathbb{P}(\text{ Sobredosis })$ si la codificación es

$$
Y = \begin{cases} 0 \quad \text{si Accidente cardiovascular}\\
1 \quad \text{si Sobredosis} 
\end{cases}
$$

## Regresión logística

Usando el mismo conjunto de datos `Default`, donde la variable respuesta `default` cae dentro de dos categorías `Yes` y `No`. En vez de modelar la respuesta $Y$ directamente, la **regresión logística** modela la probabilidad que $Y$ pertenezca a una categoría particular.

Para el conjunto de datos `Default`, la regresión logística modela la probabilidad de que haya default (morosidad). Por ejemplo, la probabilidad de default dado cierto `balance` puede ser escrito como

$$
\mathbb{P}( \text{default}=\text{Yes}|\text{balance})
$$

Los valores de esta probabilidad, que la abreviamos como $p(\text{balance})$, estarán entre 0 y 1. Por lo que para un valor particular de `balance`, se puede hacer una predicción para `default`. Por ejemplo, se podría predecir que `default=Yes` para cualquier individuo cuyo $p(\text{balance})>0.5$. Alternativamente, si una compañía quisiese ser más conservador en la predicción, podría definir $p(\text{balance})>0.1$.

### Modelo logístico

¿Cómo deberíamos modelar la relación entre $p(X)=\mathbb{P}(Y=1|X)$ y $X$?

Podemos utilizar un enfoque de regresión lineal para representar estar probabilidades, esto es:

$$
p(X)=\beta_0 + \beta_1 X
$$

Si usamos este enfoque para predecir `default=Yes` usando `balance`, entonces obtendremos el siguiente modelo (izquierda).

------------------------------------------------------------------------

![](figs/lin_reg.png)

------------------------------------------------------------------------

Para evitar lo anterior, debemos modelar $p(X)$ usando una función que entregue salidas entre 0 y 1 para todos los valores de $X$. Muchas funciones cumplen estas condiciones. En una **regresión logística**, usamos la *función logística*.

$$
p(X)=\dfrac{\exp(\beta_0 + \beta_1 X)}{1+\exp(\beta_0 + \beta_1 X)}
$$

Para ajustar el modelo anterior, usamos máxima verosimilitud

------------------------------------------------------------------------

![](figs/log_reg.png)

------------------------------------------------------------------------

Manipulando un poco la fórmula anterior, se tiene que

$$
\dfrac{p(X)}{1-p(X)}=\exp(\beta_0 + \beta_1 X)
$$

La cantidad ${p(X) \over 1-p(X)}$ se le llaman **odds**, que pueden toman cualquier valor en $\mathbb{R}^{+}$. Valores cercanos a cero y tendiendo a infinito, indican muy baja y alta probabilidad de `default`, respectivamente.

------------------------------------------------------------------------

Tomando el logaritmo en ambos lados, se tiene

$$
\log \left(\dfrac{p(X)}{1-p(X)}\right)=\beta_0 + \beta_1 X
$$

a esta cantidad la llamamos **log-odds** o **logit**. Notamos que el modelo de regresión logística tiene un logit lineal en $X$.

### Estimación de los coeficientes de regresión

Los coeficiente $\beta_0$ y $\beta_1$ en la ecuación$$
p(X)=\dfrac{\exp(\beta_0 + \beta_1 X)}{1+\exp(\beta_0 + \beta_1 X)}
$$

son desconocidos, por lo que deben ser estimados basándose en los datos de entrenamiento. Si bien podríamos ocupar una metodología de métodos cuadrados no lineales para ajustar el modelo:

$$
\log \left(\dfrac{p(X)}{1-p(X)}\right)=\beta_0 + \beta_1 X
$$

La metodología de máxima verosimilitud es usualmente preferida, debido a que tiene mejores propiedades estadísticas.

------------------------------------------------------------------------

Formalmente, definimos la **función de verosimilitud** como:

$$
\ell(\beta_0,\beta_1)=\prod_{i:y_i=1}p(x_i)\prod_{i':y_{i'}=0}(1-p(x_{i'}))
$$

Las estimaciones $\hat{\beta}_0$ y $\hat{\beta}_1$ son escogidos para maximizar la función de verosimilitud.

### Ejemplo

```{r, results='hide'}
logit <- glm(default ~ balance, data = data, family = "binomial")
summary(logit)
```

------------------------------------------------------------------------

\small

```{r, echo=FALSE}
summary(logit)
```

### Predicciones

Una vez que los coeficientes han sido estimados, lo que resta es calcular la probabilidad de `default` para una `balance` dado. Por ejemplo, la predicción para una persona con balance $\$1000$ es

$$
\hat{p}(X)=\dfrac{\exp(-10.65+ 0.0055 \times 1000)}{1+\exp(-10.65+ 0.0055 \times 1000)}\approx 0.00576
$$

que es bajo $1\%$. En contraste con alguien que adeuda $\$2000$, en cuyo casi $\hat{p}(X)=0.586$.

------------------------------------------------------------------------

Si utilizamos *dummy variables* para el predictor `student` codificado como 0 y 1. tendremos el siguiente ajuste

```{r, results='hide'}
logit_dummy<-glm(default ~ student, data = data, family = "binomial")
summary(logit_dummy)
```

------------------------------------------------------------------------

\small

```{r, echo=FALSE}
summary(logit_dummy)
```

------------------------------------------------------------------------

Así, podemos calcular las probabilidades

$$
\mathbb{P}\left( \text{default=Yes }| \text{ student=Yes}\right)=\dfrac{\exp(-3.5041+ 0.4049 \times 1)}{1+\exp(-3.5041+ 0.4049 \times 1)}\approx 0.0431
$$

y,

$$
\mathbb{P}\left( \text{default=Yes }| \text{ student=No}\right)=\dfrac{\exp(-3.5041+ 0.4049 \times 0)}{1+\exp(-3.5041+ 0.4049 \times 0)}\approx 0.0292
$$

## Regresión logística múltiple

Ahora consideramos el problema de predecir una respuesta binaria usando múltiples predictores. La extensión natural del modelo de regresión es

$$
\log \left(\dfrac{p(X)}{1-p(X)}\right)=\beta_0 + \beta_1 X_1 +\dots + \beta_p X_p
$$

donde $X=(X_1,\dots,X_p)$ son $p$ predictores. La ecuación anterior la podemos reescribir como

$$
p(X)=\dfrac{\exp(\beta_0 + \beta_1 X_1 +\dots + \beta_p X_p)}{1+ \exp(\beta_0 + \beta_1 X_1 +\dots + \beta_p X_p)}
$$

Al igual que antes, usamos método de máxima verosimilitud para estimar $\mathbf{\beta}$

### Ejemplo

```{r, results='hide'}
logit2 <- glm(default ~ balance + student + income, data = data,
             family = "binomial")
summary(logit2)
```

------------------------------------------------------------------------

\footnotesize

```{r, echo=FALSE}
summary(logit2)
```

------------------------------------------------------------------------

![Tasas de default, Estudiantes en naranjo, No-Estudiantes en azul.](figs/log_compare.pdf)

### Regresión logística para $>2$ clases en la respuesta

En el caso en que tengamos más de dos clases en la variable respuesta, es posible extender la regresión lineal. En el ejemplo de determinación de diagnóstico en una sala de emergencia se tenían las categorías accidente cardiovascular, sobredosis y ataque epiléptico, por lo que se desearía modelar

$$
\mathbb{P}\left( Y= \text{ acc. card. }| X\right)
$$

y $$
\mathbb{P}\left( Y= \text{ sobredosis }| X\right)
$$

siendo el remanente,$$
\mathbb{P}\left( Y= \text{ ataque epiléptico }| X\right)= 1-\mathbb{P}\left( Y= \text{ acc. card }| X\right)-\mathbb{P}\left( Y= \text{ sobredosis }| X\right) 
$$

Si bien es posible la extensión, en la práctica no es frecuentemente usado, pues se prefiere realizar un **análisis discriminante**.

## Análisis discriminante lineal

La regresión logística que vimos antes involucra modelar directamente $\mathbb{P}\left( Y=k|X=x\right)$ usando la función logística dada por $$
p(X)=\dfrac{\exp(\beta_0 + \beta_1 X_1 +\dots + \beta_p X_p)}{1+ \exp(\beta_0 + \beta_1 X_1 +\dots + \beta_p X_p)}
$$

para el caso de dos clases en la variable respuesta. En lo que sigue, consideramos una manera alternativa y menos directa para estimar estas probabilidades. En esta metodología, modelamos la distribución de los predictores $X$ por separado en cada una de las categorías de la variable respuesta $(Y)$, y luego usamos el teorema de Bayes para convertir estos resultados en estimaciones de $\mathbb{P}\left(Y=k|X=x\right)$.

Cuando estas distribuciones se asumen normales, la forma de este modelo es muy similar a una regresión logística.

### Teorema de Bayes para clasificación

Supongamos que queremos clasificar una observación entre $K$ clases, donde $K\geq 2$. Esto es, que la variable respuesta $Y$ puede tomar $K$ posibles valores distintos y no-ordenados.

Sea $\pi_k$ la probabilidad *apriori* que una observación escogida aleatoriamente provenga de la clase $k-$ésima. Sea $f_k(X)=\mathbb{P}(X=x|Y=k)$ la **función de densidad** de $X$ para una observación que proviene de la clase $k-$ésima. Luego, por el teorema de Bayes se tiene

$$
\mathbb{P}(Y=k|X=x)=\dfrac{\pi_k f_k(x)}{\sum_{l=1}^{K} \pi_l f_l(x)}
$$

al igual que antes usamos la notación $p_k(X)=\mathbb{P}(Y=k|X)$.

------------------------------------------------------------------------

La idea general, es estimar no estimar $p_k(X)$ directamente, sino estimar $\pi_k$ y $f_k$ para obtener lo deseado.

Usualmente $\pi_k$ es fácil de obtener si se tiene una muestra aleatoria de $Y$, pues obtenemos estas estimaciones como las proporciones de cada clase.

En cambio, estimar $f_k(X)$ tiende a ser más difícil, a menos que se asuman formas simples para las densidades.

Llamamos a la cantidad $p_k(x)$ la probabilidad *posterior* que una observación $X=x$ pertenezca a la clase $k-$ésima.

### Análisis discriminante lineal con $p=1$

Primero asumiremos que $p=1$, es decir, sólo tenemos un predictor. Deseamos obtener una estimación para $f_k(x)$ para utilizarlo en la ecuación$$
\mathbb{P}(Y=k|X=x)=\dfrac{\pi_k f_k(x)}{\sum_{l=1}^{K} \pi_l f_l(x)}
$$

y así poder estimar $p_k(x)$. Para poder estimar $f_k$, primero debemos asumir su forma, por lo que asumiremos que $f_k$ es *Gaussiana*. Por lo que,

$$
f_k(x)=\dfrac{1}{\sqrt{2\pi}\sigma_k}\exp\left( -\dfrac{1}{2\sigma_{k}^{2}}(x-\mu_k)^2\right)
$$

donde $\mu_k$ y $\sigma_{k}^{2}$ son la media y la varianza de la clase $k-$ésima. Por ahora, asumiremos que $\sigma_{1}^{2}=\dots=\sigma_{K}^{2}=\sigma^2$

------------------------------------------------------------------------

Por lo anterior, se tendrá

$$
p_k(x)=\dfrac{\pi_k \dfrac{1}{\sqrt{2\pi}\sigma}\exp\left( -\dfrac{1}{2\sigma^{2}}(x-\mu_k)^2\right)}{\sum_{l=1}^{K}\pi_l\dfrac{1}{\sqrt{2\pi}\sigma}\exp\left( -\dfrac{1}{2\sigma^{2}}(x-\mu_l)^2\right) }
$$

El clasificador Bayesiano asigna una observacion $X=x$ a la clase que su $p_k(x)$ es más grande. Si tomamos el logaritmo y arreglamos términos en la expresión anterior, se tiene que el proceso es equivalente a asignar la observación a la clase en la que

$$
\delta_k(x)=x \dfrac{\mu_k}{\sigma^2}-\dfrac{\mu_{k}^{2}}{2\sigma^2}+\log \pi_k
$$

es más grande.

------------------------------------------------------------------------

Por ejemplo, si $K=2$ Y $\pi_1=\pi_2$, entonces el clasificador Bayesiano asigna una observación a la clase 1 si $2x(\mu_1-\mu_2)>\mu_{1}^{2}-\mu_{2}^{2}$ y a la clase 2 en caso contrario. En este caso, el límite de decisión de Bayes (*Bayes decision boundary*) corresponde al punto donde

$$
x=\dfrac{\mu_{1}^{2}-\mu_{2}^{2}}{2(\mu_1-\mu_2)}=\dfrac{\mu_1+\mu_2}{2}
$$

Llamamos a este punto el punto (o área) en donde la clasificación es ambigua.

------------------------------------------------------------------------

\small

```{r}
p4<-ggplot(data.frame(x = c(-4, 4)), aes(x)) +
stat_function(fun = dnorm, args = list(mean = -1.25, sd = 1),
              color = "firebrick") + 
stat_function(fun = dnorm, args = list(mean = 1.25, sd = 1), color = "green3") +
geom_vline(xintercept = 0, linetype = "longdash") +
theme_bw()
```

------------------------------------------------------------------------

```{r,echo=FALSE}
p4
```

------------------------------------------------------------------------

El análisis discriminante lineal (LDA) aproxima el clasificador bayesiano ingresando estimaciones para $pi_k,\mu_k$ y $\sigma^2$ en $\delta_k(x)$. Particularmente, las siguientes estimaciones son usadas.

$$
\hat{\mu}_k=\dfrac{1}{n_k}\sum_{i:y_i=k}x_i
$$

y,

$$
\hat{\sigma}^{2}=\dfrac{1}{n-K}\sum_{k=1}^{K}\sum_{i:y_i=K}(x_i-\hat{\mu}_k)^2
$$

donde $n$ es el número total de observaciones en el conjunto de entrenamiento, $n_k$ es el número de observaciones en el conjunto de entrenamiento en la clase $k-$ésima.

------------------------------------------------------------------------

En el caso de que no tengamos información de $\pi_1,\dots,\pi_K$, el análisis discriminante lineal estima $\pi_k$ usando la proporción de las observaciones en el conjunto de entrenamiento que pertenece a la clase $k-$ésima. Esto es,

$$
\hat{\pi}_k=\dfrac{n_k}{n}
$$

El clasificador **LDA** reemplaza las estimaciones anteriores en $\delta_k(x)$ y asigna una observación $X=x$ a la clase en la cual $$
\hat{\delta}_k=x\dfrac{\hat{\mu}_k}{\hat{\sigma}^2}-\dfrac{\hat{\mu}_{k}^{2}}{\hat{2\sigma}^2}+\log \hat{\pi}_k
$$

es más grande. El nombre de **lineal** viene de la linealidad de la *función discriminante* $\hat{\delta}_k$ para $x$.

------------------------------------------------------------------------

\small

```{r}
set.seed(411)
grupo_a <- rnorm(n = 30, mean = -1.25, sd = 1)
grupo_b <- rnorm(n = 30, mean = 1.25, sd = 1)
datos <- data.frame(valor = c(grupo_a, grupo_b),
                    grupo = rep(c("A","B"), each = 30))

p5<-ggplot(data = datos, aes(x = valor, fill = grupo)) +
geom_histogram(alpha = 0.5, position = "identity") +
geom_vline(xintercept = 0, linetype = "longdash") +
geom_vline(xintercept = (mean(grupo_a) + mean(grupo_b))/2)  +
annotate(geom = "text", x = 1.5, y = 9, label = "Límite decisión Bayes") +
annotate(geom = "text", x = -1.5, y = 10, label = "Límite decisión LDA") +
theme_bw() + 
theme(legend.position = "top")
```

------------------------------------------------------------------------

```{r, echo=FALSE, message=FALSE, warning=FALSE}
p5
```

### Análisis discriminante lineal con $p>1$

En lo que sigue, vamos a extender las nociones de análisis discriminante cuando se tienen múltiples predictores, para ello asumiremos que $X=(X_1,X_2,\dots,X_p)$ es obtenido desde una distribución Gaussiana multivariada, con medias por clase e igual matriz de varianza-covarianza.

Recordar que si $X\sim N(\mu,\Sigma)$ con $\mathbb{E}(X)=\mu$ (vector de medias) y $Cov(X)=\Sigma$ la matriz $p\times p$ de covarianza de $X$. Formalmente, la densidad de $X$ se define como:

$$
f(x)=\dfrac{1}{(2\pi)^{p/2}|\Sigma|^{1/2}}\exp\left( -\dfrac{1}{2}(x-\mu)^{T} \Sigma^{-1}(x-\mu)\right)
$$

En el caso de $p>1$ predictores, el análisis discriminante lineal asume que las observaciones en la clase $k-$ésima son obtenidos desde una distribución normal multivariada.

------------------------------------------------------------------------

Si reemplazamos la función de densidad para la clase $k-$ésima, $f_k(X=x)$ en la ecuación

$$
\mathbb{P}(Y=k|X=x)=\dfrac{\pi_k f_k(x)}{\sum_{l=1}^{K} \pi_l f_l(x)}
$$

y usando un poco de álgebra, se puede reescribir $\delta_k(x)$ como

$$
\delta_k(x)=x^T\Sigma^{-1} \mu_k-\dfrac{1}{2}\mu_{k}^{T} \Sigma^{-1} \mu_k +\log \pi_k
$$

y el clasificador bayesiano asigna la observación $X=x$ a la clase que tienen mayor $\delta_{k}(x)$

------------------------------------------------------------------------

![Ejemplo Análisis discriminante](figs/lda_1.pdf)

------------------------------------------------------------------------

En la figura anterior, las elipses representan las regiones que contienen $95\%$ de la probabilidad de cada una de las clases. Al igual que antes, la línea punteada es el Límite de decisión Bayes. Es decir, representan el conjunto de valores $x$ para los cuales $\delta_k(x)=\delta_\ell(x)$, esto es:

$$
x^T\Sigma^{-1} \mu_k-\dfrac{1}{2}\mu_{k}^{T} \Sigma^{-1} \mu_k=x^T\Sigma^{-1} \mu_l-\dfrac{1}{2}\mu_{l}^{T} \Sigma^{-1} \mu_l
$$

para $k\neq l$.

### Ejemplo

\small

```{r}
library(MASS)
mod_lda <- lda(Species ~ Sepal.Width + Sepal.Length + Petal.Length +
                  Petal.Width, data = iris)
```

------------------------------------------------------------------------

\footnotesize

```{r}
mod_lda
```

------------------------------------------------------------------------

```{r}
predicciones <- predict(object = mod_lda, newdata = iris[, -5])
table(iris$Species, predicciones$class, dnn = c("Clase real", "Clase predicha"))
```

------------------------------------------------------------------------

```{r, eval=FALSE}
library(klaR)
partimat(Species ~ Sepal.Width + Sepal.Length + Petal.Length 
         + Petal.Width,
         data = iris, method = "lda", prec = 200,
         image.colors = c("darkgoldenrod1", "snow2", "skyblue2"),
         col.mean = "firebrick")
```

------------------------------------------------------------------------

```{r, echo=FALSE}
library(klaR)
partimat(Species ~ Sepal.Width + Sepal.Length + Petal.Length + Petal.Width,
         data = iris, method = "lda", prec = 200,
         image.colors = c("darkgoldenrod1", "snow2", "skyblue2"),
         col.mean = "firebrick")
```

## Análisis discriminante cuadrático

El **análisis discriminante cuadrático** es una alternativa a *LDA*, en la que se asumía distribución normal e igual varianza en cada una de las clases. Si bien, en el análisis discriminante cuadrático (**QDA**) también se asume que los datos provienen desde una distribución normal y estima los parámetros para predecir. Sin embargo, el **QDA** asume que cada clase tienen su propia matriz de covarianza.

Esto es, se asume que una observación proveniente de la clase $k-$ésima es de la forma $X\sim N(\mu_k,\Sigma_k)$, donde $\Sigma_k$ es la matriz de covarianza para la clase $k-$ésima.

------------------------------------------------------------------------

Bajo estos supuestos, el clasificador bayesiano asigna una observación $X=x$ a la clase en la que

```{=tex}
\begin{align*}
\delta_k(x)&=-\dfrac{1}{2}(x-\mu_k)^{T}\Sigma_{k}^{-1}(x-\mu_k)+\log \pi_k \\
&=-\dfrac{1}{2}x^{T} \Sigma_{k}^{-1}x+x^{T}\Sigma_{k}^{-1}\mu_k-\dfrac{1}{2}\mu_{k}^{T}\mu_k+\log \pi_k
\end{align*}
```
es mayor. Así, se requerirá estimar $\Sigma_k,\mu_k$ y $\pi_k$. El nombre de cuadrático viene debido a que $x$ aparece como una función cuadrática en la ecuación anterior.

### ¿LDA o QDA?

Si tenemos $p$ predictores, estimar la matriz de covarianza requiere estimar $p(p+1)/2$ parámetros. En el caso de **QDA** se estima una matriz de covarianza para cada clase, por lo que se deben estimar $Kp(p+1)/2$ parámetros. Si asumimos que las $K$ clases comparten la misma matriz de covarianza, el modelo de **LDA** es lineal en $x$, lo que significa que se debe estimar $Kp$ parámetros.

En general, el discriminante lineal es menos flexible que su contraparte cuadrática, y tiene una varianza sustancialmente menor. Sin embargo, si el supuesto de igualdad de matrices de covarianza entre las clases es erróneo, provocará que el discriminante lineal tenga un enorme sesgo.

------------------------------------------------------------------------

Usualmente, **LDA** tiende a ser mejor que **QDA** si se tienen pocas observaciones en el conjunto de entrenamiento, por lo que reducir la varianza es particularmente importante.

En contraste, **QDA** es recomendado si el conjunto de entrenamiento es grande, de manera que la varianza del clasificador no sea tan relevante, o si el supuesto de igual matriz de covarianza en las distintas clases es claramente insostenible.

------------------------------------------------------------------------

![Comparación LDA y QDA, para supuestos de igual y desigualdad de matriz de covarianza](figs/lda_qda.pdf)

## K-vecinos cercanos

Como sabemos, un clasificador Bayesiano tiene la forma:

$$
\mathbb{P}(Y=j|X=x_o)
$$

que es simplemente una probabilidad condicional. Sin embargo, tomamos este clasificador como el idóneo no-obtenible, debido a que nos entrega el error de testeo. En la práctica, este clasificar no es obtenible debido a que no sabemos la distribución condicional de $Y$ dado $X$.

------------------------------------------------------------------------

Una metodología que intenta estimar la probabilidad condicional para luego asignar la clase $k-$ésima a la observación que tenga la mayor probabilidad condicional es **K-vecinos cercanos** o **KNN** por sus sigles en inglés.

Dada un entero positivo $K$ una observación de testeo $x_0$, el clasificador $KNN$ primero identifica los $K$ puntos más cercanos a $x_0$ pertenecientes al conjunto de entrenamiento, representados por $\mathcal{N}_0$. Luego estima la probabilidad condicional para la clase $j-$ésima como una fracción de puntos en $\mathcal{N}_0$ cuyas respuestas son igual a la de $j$, esto es:

$$
\mathbb{P}(Y=j|X=x_0)=\dfrac{1}{K}\sum_{i\in \mathcal{N}_0}I(y_i = j)
$$

Finalmente, **KNN** aplica la regla de bayes y clasifica la observación de prueba/testeo $x_0$ a la clase con la mayor probabilidad

## RL vs LDA vs QDA vs KNN

Es natural preguntarse que técnica utilizar en distintas circunstancias, pues todas ellas tienen por finalidad clasificar observaciones. En lo que sigue se lista comentarios respecto a los nexos entre estas metodologías.

-   Debido a que RL y LDA producen límites de decisión lineales, usualmente entregan resultados similares.

-   Debido a los supuestos distribucionales de LDA, si estos se cumplen, suele entregar mejores resultados que una regresión logística. De no cumplirse los supuestos, la regresión puede superar a LDA.

-   KNN al tener un enfoque enteramente no-paramétrico, esto es: no asume nada sobre la forma del límite de decisión. Por lo anterior, si el límite de decisión es altamente no-lineal, KNN superará a la regresión logística y LDA. Sin embargo, no tendremos información de que predictores son importantes.

------------------------------------------------------------------------

-   QDA puede ser visto como un punto medio entre KNN y LDA/RL. Como QDA asume un límite de decisión cuadrático, puede modelar más problemas que al asumir linealidad.

-   Si bien QDA no es tan flexible como KNN, puede entregar mejores resultados bajo un número limitado de observaciones de entrenamiento debido a que se hacen ciertos supuestos sobre la forma del límite de decisión.

## Ejemplos para un mismo conjunto de datos

Utilizaremos un conjunto de datos de rendimientos porcentuales de las acciones **S&P 500** a lo largo de 1250 días, desde principios de 2001 hasta finales de 2005. Para cada día, se registraron los rendimientos porcentuals para cada uno de los 5 días hábiles previos (`lag1` a `lag5`). También se registró el volumen de acciones tranzadas en el día anterior (en billones) (variable `Volume`), el rendimiento porcentual del día en cuestión (variable `Today`) y la dirección, que representa si el mercado va hacia la alta o baja.

\small

```{r}
names(Smarket)
dim(Smarket)
```

------------------------------------------------------------------------

```{r}
head(Smarket)
```

------------------------------------------------------------------------

\small

```{r}
cor(Smarket[,-9])
```

------------------------------------------------------------------------

```{r, message=FALSE}
attach(Smarket)
p6<-ggplot(Smarket) +
  aes(x = 1:nrow(Smarket), y = Volume) +
  geom_point(shape = "circle", size = 1.5, colour = "#4682B4") +
  theme_bw()
```

------------------------------------------------------------------------

```{r}
p6
```

### Regresión logística

```{r}
glm.fit=glm(Direction~Lag1+Lag2+Lag3+Lag4+Lag5+Volume,
            data= Smarket , family = binomial )
```

------------------------------------------------------------------------

\small

```{r}
summary(glm.fit)
```

------------------------------------------------------------------------

\small

```{r}
glm.probs=predict(glm.fit,type="response")
glm.probs[1:10]
contrasts(Direction)
```

------------------------------------------------------------------------

```{r}
glm.pred=rep("Down",1250)
glm.pred[glm.probs>.5]="Up"
table(glm.pred,Direction)
(507+145)/1250
mean(glm.pred==Direction)
```

------------------------------------------------------------------------

```{r}
train=(Year <2005)
Smarket.2005=Smarket[!train,]
dim(Smarket.2005)
Direction.2005=Direction[!train]
```

------------------------------------------------------------------------

```{r}
glm.fit=glm(Direction~Lag1+Lag2+Lag3+Lag4+Lag5+Volume,
            data =Smarket,family=binomial,subset=train)
glm.probs=predict(glm.fit,Smarket.2005,type="response")
```

------------------------------------------------------------------------

```{r}
glm.pred=rep("Down",252)
glm.pred[glm.probs >.5]="Up"
table(glm.pred, Direction.2005)
mean(glm.pred==Direction.2005)
mean(glm.pred!=Direction.2005)
```

------------------------------------------------------------------------

```{r}
glm.fit=glm(Direction~Lag1+Lag2,data=Smarket,family=binomial,
            subset=train)
glm.probs=predict(glm.fit,Smarket.2005,type="response")
glm.pred=rep("Down",252)
glm.pred[glm.probs >.5]="Up"
table(glm.pred,Direction.2005)
mean(glm.pred==Direction.2005)
106/(106+76)
```

------------------------------------------------------------------------

```{r}
predict(glm.fit,newdata=data.frame(Lag1=c(1.2,1.5),Lag2=c(1.1,-0.8)),
        type="response")
```

### Análisis discriminante lineal

```{r}
lda.fit=lda(Direction~Lag1+Lag2,data=Smarket,subset=train)
lda.fit
```

------------------------------------------------------------------------

```{r}
plot(lda.fit)
```

------------------------------------------------------------------------

```{r}
lda.pred=predict(lda.fit,Smarket.2005)
names(lda.pred)
lda.class=lda.pred$class
```

------------------------------------------------------------------------

```{r}
table(lda.class,Direction.2005)
mean(lda.class==Direction.2005)
sum(lda.pred$posterior[,1] >=.5)
sum(lda.pred$posterior[,1] <.5)
```

------------------------------------------------------------------------

```{r}
lda.pred$posterior[1:20,1]
lda.class[1:20]
```

------------------------------------------------------------------------

```{r}
sum(lda.pred$posterior[,1] >.9)
```

### Análisis discriminante cuadrático

```{r}
qda.fit=qda(Direction~Lag1+Lag2,data=Smarket,subset=train)
qda.fit
```

------------------------------------------------------------------------

```{r}
qda.class=predict(qda.fit,Smarket.2005)$class
table(qda.class,Direction.2005)
mean(qda.class==Direction.2005)
```

### K-vecinos cercanos

```{r,warning=FALSE}
library(class)
train.X=cbind(Lag1,Lag2)[train,]
test.X=cbind(Lag1,Lag2)[!train,]
train.Direction=Direction[train]
```

------------------------------------------------------------------------

```{r}
knn.pred=knn(train.X,test.X,train.Direction,k=1)
table(knn.pred,Direction.2005)
(83+43)/252
```

------------------------------------------------------------------------

```{r}
knn.pred=knn(train.X,test.X,train.Direction,k=3)
table(knn.pred,Direction.2005)
mean(knn.pred==Direction.2005)
```

### Ejercicio

Realizar **KNN** con el conjunto `Caravan` y comparar su desempeño con un modelo de regresión logística.

## Árbol de decisión

Los árboles de decisión son una metodología que estratifica o segmenta el espacio de los predictores en distintas regiones, en donde se utiliza una serie de reglas de división para segmentar los espacios.

En general, esta metodología no es lo suficientemente competitiva en contraste con otras técnicas supervisadas (en términos de su precisión).

El árbol de decisión puede ser aplicada para problemas de regresión y clasificación, en lo que sigue sólo nos concentramos en esta metodología para problemas de clasificación.

------------------------------------------------------------------------

En un árbol de decisión para problemas de clasificación se predice que cada observación pertenece a la clase más frecuente entre las observaciones de entrenamiento en la región en la que pertenece.

Utilizamos la **tasa de error de clasificación** para separar los espacios a lo largo del árbol de decisión. Debido a que se planea asignar una observación en una región particular a la *clase más frecuente* en el conjunto de entrenamiento, este error se define como:

$$
E=1-\max_{k}(\hat{p}_{mk})
$$

en donde $\hat{p}_{mk}$ representa la proporción de observaciones de entrenamiento en la región $m-$ésima que son de la clase $k-$ésima.

------------------------------------------------------------------------

![Árbol de decisión para conjunto de datos Hitters](figs/tree1.pdf)

------------------------------------------------------------------------

En general, usar sólo la tasa de error de clasifición no es lo suficientemente sensible para esta metodología, y se opta por dos medidas alternativas: índice de Gini y entropía cruzada.

El **índice de Gini** está definido como:

$$
G=\sum_{k=1}^{K} \hat{p}_{mk}(1-\hat{p}_{mk})
$$

que es una medida de la varianza total a lo largo de las $K$ clases. Es claro ver que este índice toma valores pequeños si todos los $\hat{p}_{mk}$ son cercanos a cero.

------------------------------------------------------------------------

Una alternativa al índice anterior es la **entropía cruzada**, dada por:

$$
D=-\sum_{k=1}^{K} \hat{p}_{mk}\log \hat{p}_{mk}.
$$

Debido a que $0 \leq \hat{p}_{mk}\leq 1$, sigue que $0\leq -\hat{p}_{mk}\log\hat{p}_{mk}$. Se puede mostrar que la entropía cruzada tomará valores cercanos a cero si todos los $\hat{p}_{mk}$ están cercano a cero o a uno. Por lo que ambos índices tomaran valores pequeños si la $m-$ésimo *nodo* es *puro*.

------------------------------------------------------------------------

![](figs/tree2.pdf)

------------------------------------------------------------------------

```{r, warning=FALSE,message=FALSE}
library(tree)
attach(Carseats)
require(ISLR)
```

------------------------------------------------------------------------

```{r}
head(Carseats)
```

------------------------------------------------------------------------

```{r,warning=FALSE}
High=as.factor(ifelse(Sales<=8,"No","Yes"))
Carseats=data.frame(Carseats,High)
Carseats_tree=tree(High~ . -Sales, data=Carseats)
```

------------------------------------------------------------------------

```{r}
summary(Carseats_tree)
```

------------------------------------------------------------------------

```{r}
plot(Carseats_tree)
text(Carseats_tree, pretty=0)
```

------------------------------------------------------------------------

```{r, warnings=FALSE}
#Alternativa
library(rpart)
library(rpart.plot)
Carseats_tree2<-rpart(formula=High~ . -Sales, data=Carseats)
```

------------------------------------------------------------------------

```{r}
tree_plot<-rpart.plot(Carseats_tree2)
```

------------------------------------------------------------------------

```{r}
train=sample(1:nrow(Carseats), 200)
Carseats_test=Carseats[-train,]
High_test=High[-train]
Carseats_tree=tree(High~ .-Sales ,Carseats ,subset = train)
tree_pred=predict(Carseats_tree ,Carseats_test , type ="class")
table(tree_pred,High_test)
(92+65)/200
```

## Máquina de vectores de soporte (SVM)

Esta metodología fue desarrollada en lo '90 por la comunidad de ciencias computacionales. Consiste en la generalización de un clasificador llamado **clasificador de máximo margen**, que destaca por su simpleza, pero que en la práctica es difícil de utilizar debido a que requiere que las clases sean separables por un límite lineal.

### Clasificador de máximo margen

Antes de definidir el clasificador de máximo margen, debemos introducir dos conceptos primordiales:

-   Hiperplano

-   Hiperplano de separación óptimo

### Hiperplano

En un espacio $p-$dimensional, un hiperplano es un subespacio afín plano de dimensión $p-1$. Por ejemplo, en dos dimensiones, un hiperplano es una linea. En tres dimensiones, un hiperplano es un subespacio 2-dimensional plano.

Matemáticamente, para el caso bidimensional, un hiperplano está definido por la ecuación:

$$
\beta_0+\beta_1 X_1 + \beta_2 X_2 = 0
$$

para parámetros $\beta_0,\beta_1$ y $\beta_2$. Naturalmente, la extensión a $p$ dimensiones es:

$$
\beta_0+\beta_1 X_1 +\beta_2 X_2 + \dots + \beta_p X_p =0
$$

que define un hiperplano $p-$dimensional, en el sentido de que si un punto $X=(X_1,X_2,\dots,X_p)^T$ en un espacio $p-$dimensional que satisface la ecuación anterior.

------------------------------------------------------------------------

![Hiperplano \$1+2X_1+3X_2=0\$](figs/hyperplane.pdf)

### Clasificando usando un hiperplano separable

Supongamos que tenemos una matriz de datos $\mathbf{X}$ de tamaño $x\times p$ que consiste en $n$ observaciones de entrenamiento en un espacio $p-$dimensional,

$$
x_1=\begin{pmatrix}x_{11} \\ \vdots \\ x_{1p}\end{pmatrix} , \dots,x_n=\begin{pmatrix} x_{n1} \\ \vdots \\ x_{np} \end{pmatrix}
$$

y que estas observaciones caen dentro de dos clases, esto es, $y_1,\dots,y_n \in \{-1,1\}$ donde $-1$ representa una clase y $1$ la otra clase. También tenemos una observación de prueba, un $p-$vector de *features* observadas $x^*=(x_{1}^{*}\, \dots \,x_{p}^{*})^T$

------------------------------------------------------------------------

Nuestro objetivo es desarrollar un clasificador basado en este conjunto de entrenamiento que clasifique correctamente la observación de prueba usando las variables medidas, para esto nosotros ya hemos visto varias metodologías que podríamos usar: LDA, QDA, árboles de decisión y regresión logística.

En lo que sigue veremos una metodología que se base en el concepto de hiperplano separable.

------------------------------------------------------------------------

Supongamos que es posible construir un hiperplano que separe las observaciones de entrenamiento perfectamente de acuerdo a sus clases. Por lo que si utilizamos las clase como antes ($\{-1,1\}$) se tendrá la propiedad que

$$
\beta_0+\beta_1 x_{i1} +\beta_2 x_{i2}+\dots+\beta_p x_{ip} > 0 \quad \text{si} \quad y_i=1
$$

y,

$$
\beta_0+\beta_1 x_{i1} +\beta_2 x_{i2}+\dots+\beta_p x_{ip} < 0 \quad \text{si} \quad y_i=-1
$$

equivalentemente, un hiperplano separable tiene la propiedad que:

$$
y_i(\beta_0 +\beta_0+\beta_1 x_{i1} +\beta_2 x_{i2}+\dots+\beta_p x_{ip})>0
$$

para todo $i=1,\dots,n$.

------------------------------------------------------------------------

![Hiperplanos separables](figs/hyperplane_sep.pdf)

------------------------------------------------------------------------

Si un hiperplano separable existe, podemos usarlo para construir un clasificador bastante natural: una observación de prueba es asignada una clase dependiendo de que lado del hiperplano está ubicada.

Intuitivamente, podremos estar seguro de nuestra clasificación conforme la magnitud obtenida tras clasificar la observación de prueba.

### Clasificador de margen máximo

En general, si nuestros datos pueden ser perfectamente separados un hiperplano, entonces existiran un infinito número de aquellos hiperplanos. Para poder construir un clasificador basado en un hiperplano separable, debemos encontrar una forma razonable de decidir cual de estos infinitos hiperplanos separables usar.

Una elecci ón natural es el **hiperplano de máximo margen** (también conocido como *hiperplano separable máximo*), que es el hiperplano que está más lejos de las observaciones de entrenamiento. Esto es, podemos calcular la distancia (perpendicular) desde cada punto a un hiperplano separable dado; la menor de aquellas distancias es la mínima distancia entre las observaciones y el hiperplano, esta distancia es conocida como **margen**.

------------------------------------------------------------------------

El hiperplano de margen máximo es el hiperplano separable en donde el margen es el más grande, esto es, es el hiperplano que tiene la distancia mínima más lejana a las observaciones de entrenamiento.

Luego, podemos clasificar una observación de prueba basado en que lado del hiperplano de margen máximo recae.

------------------------------------------------------------------------

![Representación de vectores de suporte.](figs/support_vectors.pdf)

### Construcción de un clasificador de margen máximo

Ahora consideramos la tarea de construir el hiperplano de margen máximo basado en un conjunto de $n$ observaciones de entrenamiento $x_1,\dots, x_n \in \mathbb{R}^{p}$ y clases asociadas $y_1,\dots,y_n \in \{-1,1\}$. En síntesis, este hiperplano es la solución de un problema de optimización dado por:

```{=tex}
\begin{align*}
&\max_{\beta_0,\beta_1,\dots,\beta_p} \quad M\\
&\text{ Sujeto a } \sum_{j=1}^{p} \beta_{j}^{2}=1 \\
&y_i(\beta_0 +\beta_0+\beta_1 x_{i1} +\beta_2 x_{i2}+\dots+\beta_p x_{ip})\geq M \quad \forall i=1,\dots,n
\end{align*}
```

------------------------------------------------------------------------

```{r}
library(e1071)
library(ggplot2)
set.seed(411)
coord <- matrix(rnorm(40), 20, 2)
colnames(coord) <- c("X1","X2")
y <- c(rep(-1,10), rep(1,10))
coord[y == 1, ] <- coord[y == 1, ] + 1
data <- data.frame(coord, y)
plot_svm<-ggplot(data = data, aes(x = X1, y = X2, color = as.factor(y))) +
  geom_point(size = 6) +
  theme_bw() +
  theme(legend.position = "none")

data$y <- as.factor(data$y)
```

------------------------------------------------------------------------

\footnotesize

```{r}
mod_svm <- svm(formula = y ~ X1 + X2, data = data, kernel = "linear",
                  cost = 10, scale = FALSE)
summary(mod_svm)
```

------------------------------------------------------------------------

```{r}
mod_svm$index
```

------------------------------------------------------------------------

```{r}
plot(mod_svm, data)
```

### Clasificador de vectores de soporte

![Dos clases no separables por hiperplano](figs/hyperplane_non_sep.pdf)

------------------------------------------------------------------------

En el caso anterior, un hiperplano separable no existe. En la figura siguiente, la adición de un solo dato provoca un cambio drástico en el margen máximo del hiperplano.

![](figs/hyperplane_non_sep_2.pdf)

------------------------------------------------------------------------

Este cambio, reduce la confianza de la asignación de clases. Así, podemos concluir que nuestra metodología es extremadamente sensible a cambio, incluso de sólo una observación.

En estos casos, quizás deberíamos considerar un clasificador basado en un hiperplano que no separe perfectamente las dos clases, con el fin de:

-   Tener mayor robustez a las observaciones individuales

-   Tener mayor clasificación para la mayoría de las observaciones de entrenamiento

------------------------------------------------------------------------

Así, podría ser beneficioso clasificar erróneamente un par de observaciones de entrenamiento para realizar un mejor trabajo clasificando el resto de las observaciones.

El Clasificador de vectores de soporte (*support vector classifier*) o aveces llamado *soft margin classifier*, hace exactamente lo anterior; en vez de buscar el marger más grande tal que cada observación que clasifique perfectamente, permite que ciertas observaciones estén en la lado incorrecto del margen, o del lado incorrecto del hiperplano.

------------------------------------------------------------------------

Matemáticamente, corresponde a la solución del siguiente problema de optimización:

```{=tex}
\begin{align*}
&\max_{\beta_0,\beta_1,\dots,\beta_p; \epsilon_1,\dots,\epsilon_n} \quad M\\
&\text{ Sujeto a } \sum_{j=1}^{p} \beta_{j}^{2}=1 \\
&y_i(\beta_0 +\beta_0+\beta_1 x_{i1} +\beta_2 x_{i2}+\dots+\beta_p x_{ip})\geq M (1-\epsilon_i) \quad \forall i=1,\dots,n \\
&\epsilon_i \geq 0, \sum_{i=1}^{n} \epsilon_i \leq C
\end{align*}
```
donde $C$ es un parámetro de *tunning* no negativo. Cuando este parámetro es grande, habrá una gran tolerancia a que las observaciones están al lado incorrecto del margen (y por ende el margen será grande)

### SVM

Support Vector Machine (SVM) o Maquina de vectores de soporte es una extensión del clasificador de vectores de soporte que se obtiene tras aumentar el espacio de variables de una manera específica: usando **kernels**.

En el caso del problema de optimización del clasificador de vectores de soporte, la solución involucra sólo **productos internos** de las observaciones (en contraste con las observaciones mismas).

------------------------------------------------------------------------

El producto interno de dos $r-$vectores $a$ y $b$ se define como $\langle a,b\rangle=\sum_{i=1}^{r} a_i b_i$. Por lo que el producto interno de dos observaciones $x_i,x_{i'}$ está dado por:

$$
\langle x_i, x_{i'}\rangle=\sum_{j=1}^{p}x_{ij}x_{i'j}
$$

Más precisamente, se puede mostrar que:

-   El clasificador de vectores de soportes lineal se puede representar como

    $$
    f(x)=\beta_0+\sum_{i=1}^{n}\alpha_i\langle x,x_i\rangle
    $$

donde hay $n$ parámetros $\alpha_i, i=1,\dots,n$, uno para cada observación de entrenamiento.

------------------------------------------------------------------------

-   Para estimar los parámetros $\alpha_1,\dots,\alpha_n$ y $\beta_0$, sólo necesitamos los $\begin{pmatrix} n \\ 2 \end{pmatrix}$ productos internos $\langle x_i,x_{i'}\rangle$ entre todos los pares de observaciones de entrenamiento. (esto es $n(n-1)/2$ pares).

Supongamos que cada producto interno que hemos definido aparece en la representación del clasificador de vectores de soporte lineal, o en el cálculo del problema de optimización. Reemplazaremos este producto interno con una **generalización** de este, de la forma:

$$
K(x_i,x_{i'})
$$

donde $K$ es una función que le llamaremos **kernel**.

------------------------------------------------------------------------

Un **kernel** es una función que cuantifica la similitud entre dos observaciones. Por ejemplo, podemos tomar:

$$
K(x_i,x_{i'})=\sum_{j=1}^{p} x_{ij}x_{i'j}
$$

que nos entregaría el clasificar de vectores de soporte. Lo anterior se dice que es un kernel lineal porque el lineal para las *features*. El kernel lineal esencialmente cuantifica la similitud de un par de observaciones usando la correlación de Pearson.

------------------------------------------------------------------------

Alternativamente, podemos considerar otra forma de kernel, por ejemplo:

$$
K(x_i,x_{i'})=(1+\sum_{j=1}^{p} x_{ij}x_{i'j})^{d}
$$

Este kernel es conocido como el **kernel polinomial** de grado $d$, donde $d$ es un entero positivo. Cuando este parámetro es mayor a 1, el clasificador de vectores de soportes tiende a tener un límite de decisión bastante más flexible.

------------------------------------------------------------------------

Cuando el clasificador de vectores de soporte es combinado con un kernel no lineal (como el anterior), el clasificador resultante se conoce como **support vector machine**.

Otra opción popular de kernel es el **kernel radial**, que tiene la forma:

$$
K(x_i,x_{i'})=\exp(-\gamma \sum_{j=1}^{p}(x_{ij}-x_{i'j})^2)
$$

donde $\gamma$ es una constante positiva.

------------------------------------------------------------------------

![Ejemplo de SVM con kernel polinomial de grado 3 y SVM con kernel radial.](figs/radial_kernel.pdf)

### Ejemplo

```{r}
set.seed (411)
x=matrix(rnorm(200*2), ncol=2)
x[1:100,]=x[1:100,]+2
x[101:150,]=x[101:150,]-2
y=c(rep(1,150),rep(2,50))
dat=data.frame(x=x,y=as.factor(y))
plot_radial<-ggplot(dat) +
  aes(x = x.1, y = x.2, colour = y) +
  geom_point(shape = "circle", size = 1.5) +
  scale_color_hue(direction = 1) +
  theme_bw()
```

------------------------------------------------------------------------

\small

```{r}
plot_radial
```

------------------------------------------------------------------------

```{r}
train=sample(200,100)
svmfit=svm(y~., data=dat[train,], kernel="radial", gamma=1,cost =1)
plot(svmfit , dat[train ,])
```

------------------------------------------------------------------------

\small

```{r}
summary(svmfit)
```

------------------------------------------------------------------------

```{r}
table(true=dat[-train,"y"],pred=predict(svmfit, newdata=dat[-train,]))
```

# Métodos no supervisados

Como hemos mencionado antes, en los métodos no supervisados **sólo** tenemos las variables $X_1,X_2,\dots,X_p$ medidas en $n$ observaciones. No estaremos interesados en predecir, porque no tenemos una variable respuesta $Y$ que esté asociada a nuestros datos. En cambio, nuestro objetivo será *descubrir* características interesantes de las variables medidas.

-   ¿Hay alguna forma que nos entregue información de visualizar los datos?

-   ¿Podemos encontrar subgrupos entre las variables u observaciones?

En lo que sigue nos concentraremos en 3 técnicas particulares: *clustering*, análisis de componentes principales y descomposición de valores singulares.

------------------------------------------------------------------------

En general, realizar técnicas no supervisadas tiende a ser más difícil que realizar un método supervisado, pues no se tiene un objetivo claro para el análisis (como lo es predecir en el caso supervisado).

Usualmente, las metodologías no supervisadas se realizar como parte del análisis exploratorio de dato. Además, no existe un consenso en la mejor forma de evaluar las técnicas implementadas en datos de prueba. En el caso supervisados, podemos probar nuestro modelo creado con un conjunto de prueba, pero en el caso no supervisado no es posible debido a que no sabemos el respuesta **verdadera**.

------------------------------------------------------------------------

Ejemplos de aplicación de estas metodologías son vastas:

-   Identificación de cáncer

-   Historiales de compras

-   Etc.

## Análisis de componentes principales

El análisis de componentes principales nos permite resumir, ante un conjunto grande de variables correlacionadas, un subconjunto con menor número de variables representativas que colectivamente explican la mayoría de la variabilidad del conjunto original.

Además de producir variables que pueden ser usadas para métodos supervisados, **PCA** (por sus siglas en inglés, *principal component analysis*) sirve como herramienta para visualizar datos.

### Qué son las componentes principales?

Supongamos que deseamos visualizar $n$ observaciones con mediciones en un conjunto de $a$ variables/características/*features*, $X_1,X_2,\dots,X_p$, como parte de un análisis exploratorio de datos. Podemos examinar los gráficos bidimensionales de dispersión de los datos, que cada una contiene $n$ mediciones de observaciones de 2 variables. Sin embargo, hay $\begin{pmatrix} p \\ 2 \end{pmatrix}=p(p-1)/2$ de tales gráficos.

Por ejemplo, para $p=10$ habrán 45 gráficos. Por lo que si, $p$ es grande no nos será posible mostrarlos todos, y además ninguno de ellos será informativo debido a que sólo tienen un pequeña fracción del total de información disponible en los datos.

Así, necesitamos una mejor metodología para poder visualizar las $n$ observaciones cuando $p$ es grande.

------------------------------------------------------------------------

En particular, quisieramos encontrar una presentación de baja dimensionalidad de los datos que capture la mayor cantidad de información posible. Por ejemplo, si podemos obtener un diagrama bidimensional de los datos que capture la mayoría de la información, entonces podemos graficar las observaciones en aquel espacio.

PCA nos entrega una herramienta para hacer justamento esto. Encuentra una representación de baja dimensionalidad del conjunto de datos que contiene la mayor variabilidad posible. La idea es que cada una de las $n$ observaciones vive en un espacio $p-$dimensional, pero no todas estas dimensiones son igualmente interesantes.

------------------------------------------------------------------------

PCA busca un pequeño número de dimensiones que sean lo más interesantes posibles, donde el concepto de *interesante* es medido por la cantidad que varían las observaciones en cada uno de las dimensiones.

Cada una de las dimensiones encontradas por PCA es una combinación lineal de $p$ variables.

Ahora nos enfocamos en la manera en que PCA encuentra estas dimensiones o componentes principales.

------------------------------------------------------------------------

El **primer componente principal** de un conjunto de variables $X_1,X_2,\dots,X_p$ es la combinación lineal normalizada de las variables$$
Z_1=\phi_{11}X_1+\phi_{21}X_2+\dots+\phi_{p1}X_p
$$

que tenga la **mayor varianza**. Por *normalizada*, se refiere a que

$$
\sum_{j=1}^{p} \phi_{j1}^{2}=1.
$$

Llamamos a los elementos $\phi_{11},\phi_{2 1},\dots,\phi_{p1}$reciben en el nombre de *loadings* y son los que definen a la componente. Así, estos elementos conforman el vector de *loadings* de los componentes principales $\phi_1=(\phi_{11}\,\phi_{21}\dots\phi_{p1})'$

------------------------------------------------------------------------

Dado un conjunto de datos $\mathbf{X}$ de tamaño $n\times p$. ¿Cómo calculamos la primera componente principal?

Debido a que sólo estamos interesado en la varianza, asumiremos que cada variable de $\mathbf{X}$ ha sido centrada en cero ( esto es, que las medias de las columnas sean cero). Luego, buscamos la combinación lineal de las variables medidas con forma:

$$
z_{i1}=\phi_{11}x_{i1}+\phi_{21}x_{i2}+\dots+\phi_{p1}x_{ip}
$$

que tenga la mayor varianza muestral, sujeto a la restricción que $\sum_{j=1}^{p} \phi_{j1}^{2}=1$. En otras palabras, el vector de *loadings* de la primera componente principal resuelve el siguiente problema de optimización:

$$\max_{\phi_{11},\dots,\phi_{p1}}\Bigg\{ \dfrac{1}{n}\sum_{i=1}^{n} \left( \sum_{j=1}^{p} \phi_{j1}x_{ij}\right)\Bigg\} \text{ sujeto a }\sum_{j=1}^{p}\phi_{j1}^2=1
$$

### Reproducibilidad de las componentes

El proceso de PCA genera siempre las mismas componentes principales independientemente del software utilizado, es decir, el valor de los *loadings* resultantes es el mismo.

La única discrepancia que podría suceder es que los signos estén invertidos, pues los *loadings* determinan la dirección de la componente.

### Influencia de outliers

Al trabajar con varianzas, el método PCA es altamente sensible a *outliers*, por lo que es altamente recomendable estudiar si los hay. La detección de valores atípicos con respecto a una determinada dimensión es algo relativamente sencillo de hacer mediante comprobaciones gráficas.

Las técnicas diagnóstico de datos anómalos escapa de los objetivos del curso, pero son estudiados en análisis multivariado o modelos lineales (dentro del contexto de regresión)

### Proporción de varianza explicada

Una de las preguntas más frecuentes que surge tras realizar un PCA es: ¿Cuánta información presente en el set de datos original se pierde al proyectar las observaciones en un espacio de menor dimensión? o lo que es lo mismo ¿Cuanta información es capaz de capturar cada una de las componentes principales obtenidas? Para contestar a estas preguntas se recurre a la proporción de varianza explicada por cada componente principal.

Asumiendo que las variables se han estandarizado para tener media cero, la varianza total presente en el set de datos se define como:

$$
\sum_{j=1}^p Var(X_j) = \sum_{j=1}^p \dfrac{1}{n} \sum_{i=1}^n x^{2}_{ij}
$$

------------------------------------------------------------------------

y la varianza explicada por la componente $m$ es:

$$
\dfrac{1}{n} \sum_{i=1}^n z^{2}_{im} = \dfrac{1}{n} \sum_{i=1}^n  \left( \sum_{j=1}^p \phi_{jm}x_{ij} \right)^2
$$

Así, la proporción de varianza explicada por la componente $m$ viene dada por

$$
\dfrac{\sum_{i=1}^n  \left( \sum_{j=1}^p \phi_{jm}x_{ij} \right)^2} {\sum_{j=1}^p \sum_{i=1}^n x^{2}_{ij}}
$$

Esta proporción y su forma acumulada (a lo largo de las componentes) nos entrega información crucial a la hora de elegir cuantas componentes principales utilizar en nuestro análisis.

### Número óptimo de componentes principales

Por lo general, dada una matriz de datos de dimensiones $n \times p$, el número de componentes principales que se pueden calcular es como máximo de $\min\{n-1,p\}$. Sin embargo, siendo el objetivo del PCA reducir la **dimensionalidad**, suelen ser de interés utilizar el número mínimo de componentes que resultan suficientes para explicar los datos.

No existe una respuesta o método único que permita identificar cual es el número óptimo de componentes principales a utilizar. Una forma de proceder muy extendida consiste en evaluar la proporción de varianza explicada acumulada y seleccionar el número de componentes mínimo a partir del cual el incremento deja de ser sustancial.

------------------------------------------------------------------------

![](figs/optimal_pca.png)

### Ejemplo

Datos del porcentaje de asaltos, asesinatos y secuestros por cada 100 mil habitantes para cada uno de los estos de US, en el año 1973. Adicionalmente, se incluye el porcentaje de la población de cada estado que vive en zonas rurales.

```{r}
data("USArrests")
head(USArrests)
```

------------------------------------------------------------------------

```{r}
apply(X = USArrests, MARGIN = 2, FUN = mean)
apply(X = USArrests, MARGIN = 2, FUN = var)
```

Si no estandarizamos, la variable *Assault* será la que dominará nuestro PCA.

------------------------------------------------------------------------

```{r}
pca <- prcomp(USArrests, scale = TRUE)
names(pca)
```

------------------------------------------------------------------------

Elementos *center* y *scale* contienen la media y desviación en escala original.

```{r}
pca$center
pca$scale
```

------------------------------------------------------------------------

Elemento *rotation* contiene el valor de los loadings para cada componente

```{r}
pca$rotation
```

------------------------------------------------------------------------

Valor de las componentes principales para cada observación (*principal component scores*)

```{r}
head(pca$x)
dim(pca$x)
```

------------------------------------------------------------------------

```{r}
biplot(x = pca, scale = 0, cex = 0.6, col = c("blue4", "brown3"))
```

------------------------------------------------------------------------

```{r}
pca$rotation <- -pca$rotation
pca$x        <- -pca$x
biplot(x = pca, scale = 0, cex = 0.6, col = c("blue4", "brown3"))
```

------------------------------------------------------------------------

```{r}
pca$sdev^2
prop_varianza <- pca$sdev^2 / sum(pca$sdev^2)
prop_varianza
```

------------------------------------------------------------------------

```{r}
ej_pca<-ggplot(data = data.frame(prop_varianza, pc = 1:4),
       aes(x = pc, y = prop_varianza)) +
  geom_col(width = 0.3) +
  scale_y_continuous(limits = c(0,1)) +
  theme_bw() +
  labs(x = "Componente principal",
       y = "Prop. de varianza explicada")
```

------------------------------------------------------------------------

```{r}
ej_pca
```

------------------------------------------------------------------------

```{r}
prop_varianza_acum <- cumsum(prop_varianza)
prop_varianza_acum
```

------------------------------------------------------------------------

```{r}
ej_pca2<-ggplot(data = data.frame(prop_varianza_acum, pc = 1:4),
       aes(x = pc, y = prop_varianza_acum, group = 1)) +
  geom_point() +
  geom_line() +
  theme_bw() +
  labs(x = "Componente principal",
       y = "Prop. varianza explicada acumulada")
```

------------------------------------------------------------------------

```{r}
ej_pca2
```

## Métodos de agrupamiento

TBD
